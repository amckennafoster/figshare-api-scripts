{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "thorough-dictionary",
   "metadata": {},
   "source": [
    "## Bulk upload OA paper metadata records from Dimensions JSON export\n",
    "This formats the json export from Dimensions for upload to Figshare. This uses the create private article API endpoint: https://docs.figshare.com/#private_article_create\n",
    "\n",
    "\n",
    "All records are created as linked file records.\n",
    "\n",
    "Here are the steps:\n",
    "1. Open a json file\n",
    "2. Pull out the relevant fields and give them the proper keys (account for partial dates, author formatting,and missing abstracts)\n",
    "3. Interate through and upload the records\n",
    " - convert the json record to a string with double quotes\n",
    " - upload the record\n",
    " - log the api response details if it fails\n",
    " - update the author list of the new record (removes the admin account as an author). The create record response returns the api endpoint for updating.\n",
    " - Add the existing DOI as a linked file\n",
    "4. This can upload to a specific group with specific custom metadata. You can change the api key to upload to different accounts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-williams",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e37a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-lending",
   "metadata": {},
   "source": [
    "## Set token and descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continued-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the token in the header and base URL\n",
    "\n",
    "text_file = open(\"../../../testing-token-symlilliput.txt\", \"r\")\n",
    "TOKEN = text_file.read()\n",
    "TOKEN.strip() #removes any hidden spaces\n",
    "text_file.close()\n",
    "\n",
    "#TOKEN = str(ENTER TOKEN HERE WITH QUOTES)\n",
    "\n",
    "api_call_headers = {'Authorization': 'token ' + TOKEN}\n",
    "\n",
    "#Set the base URL\n",
    "BASE_URL = 'https://api.figsh.com/v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-harbor",
   "metadata": {},
   "source": [
    "## Load the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open a file if you have one\n",
    "#with open(\"CMU-pubs.json\", \"r\", encoding='utf8') as read_file: #Replace this with the filename of your choice\n",
    "#    jsonfile = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "handmade-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For demonstration, create a json file with 2 records- this is json formatted metadata from the Dimensions API\n",
    "jsonfile = [\n",
    "\t{\n",
    "\t\t\"abstract\": \"As robots are deployed to work in our environments, we must build appropriate expectations of their behavior so that we can trust them to perform their jobs autonomously as we attend to other tasks. Many types of explanations for robot behavior have been proposed, but they have not been fully analyzed for their impact on aligning expectations of robot paths for navigation. In this work, we evaluate several types of robot navigation explanations to understand their impact on the ability of humans to anticipate a robot’s paths. We performed an experiment in which we gave participants an explanation of a robot path and then measured (i) their ability to predict that path, (ii) their allocation of attention on the robot navigating the path versus their own dot-tracking task, and (iii) their subjective ratings of the robot’s predictability and trustworthiness. Our results show that explanations do significantly affect people’s ability to predict robot paths and that explanations that are concise and do not require readers to perform mental transformations are most effective at reducing attention to the robot.\",\n",
    "\t\t\"authors\": [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"affiliations\": [\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"city\": \"Pittsburgh\",\n",
    "\t\t\t\t\t\t\"city_id\": 5206379,\n",
    "\t\t\t\t\t\t\"country\": \"United States\",\n",
    "\t\t\t\t\t\t\"country_code\": \"US\",\n",
    "\t\t\t\t\t\t\"id\": \"grid.147455.6\",\n",
    "\t\t\t\t\t\t\"name\": \"Carnegie Mellon University\",\n",
    "\t\t\t\t\t\t\"raw_affiliation\": \"Carnegie Mellon University, Pittsburgh, PA, USA\",\n",
    "\t\t\t\t\t\t\"state\": \"Pennsylvania\",\n",
    "\t\t\t\t\t\t\"state_code\": \"US-PA\"\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t],\n",
    "\t\t\t\t\"corresponding\": \"\",\n",
    "\t\t\t\t\"current_organization_id\": \"\",\n",
    "\t\t\t\t\"first_name\": \"Stephanie\",\n",
    "\t\t\t\t\"last_name\": \"Rosenthal\",\n",
    "\t\t\t\t\"orcid\": [],\n",
    "\t\t\t\t\"raw_affiliation\": [\n",
    "\t\t\t\t\t\"Carnegie Mellon University, Pittsburgh, PA, USA\"\n",
    "\t\t\t\t],\n",
    "\t\t\t\t\"researcher_id\": \"null\"\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"affiliations\": [\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"city\": \"Pittsburgh\",\n",
    "\t\t\t\t\t\t\"city_id\": 5206379,\n",
    "\t\t\t\t\t\t\"country\": \"United States\",\n",
    "\t\t\t\t\t\t\"country_code\": \"US\",\n",
    "\t\t\t\t\t\t\"id\": \"grid.147455.6\",\n",
    "\t\t\t\t\t\t\"name\": \"Carnegie Mellon University\",\n",
    "\t\t\t\t\t\t\"raw_affiliation\": \"Carnegie Mellon University, Pittsburgh, PA, USA\",\n",
    "\t\t\t\t\t\t\"state\": \"Pennsylvania\",\n",
    "\t\t\t\t\t\t\"state_code\": \"US-PA\"\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t],\n",
    "\t\t\t\t\"corresponding\": \"\",\n",
    "\t\t\t\t\"current_organization_id\": \"\",\n",
    "\t\t\t\t\"first_name\": \"Peerat\",\n",
    "\t\t\t\t\"last_name\": \"Vichivanives\",\n",
    "\t\t\t\t\"orcid\": [],\n",
    "\t\t\t\t\"raw_affiliation\": [\n",
    "\t\t\t\t\t\"Carnegie Mellon University, Pittsburgh, PA, USA\"\n",
    "\t\t\t\t],\n",
    "\t\t\t\t\"researcher_id\": \"null\"\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"affiliations\": [\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"city\": \"Pittsburgh\",\n",
    "\t\t\t\t\t\t\"city_id\": 5206379,\n",
    "\t\t\t\t\t\t\"country\": \"United States\",\n",
    "\t\t\t\t\t\t\"country_code\": \"US\",\n",
    "\t\t\t\t\t\t\"id\": \"grid.147455.6\",\n",
    "\t\t\t\t\t\t\"name\": \"Carnegie Mellon University\",\n",
    "\t\t\t\t\t\t\"raw_affiliation\": \"Carnegie Mellon University, Pittsburgh, PA, USA\",\n",
    "\t\t\t\t\t\t\"state\": \"Pennsylvania\",\n",
    "\t\t\t\t\t\t\"state_code\": \"US-PA\"\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t],\n",
    "\t\t\t\t\"corresponding\": \"\",\n",
    "\t\t\t\t\"current_organization_id\": \"grid.147455.6\",\n",
    "\t\t\t\t\"first_name\": \"Elizabeth\",\n",
    "\t\t\t\t\"last_name\": \"Carter\",\n",
    "\t\t\t\t\"orcid\": [\n",
    "\t\t\t\t\t\"0000-0002-2735-148X\"\n",
    "\t\t\t\t],\n",
    "\t\t\t\t\"raw_affiliation\": [\n",
    "\t\t\t\t\t\"Carnegie Mellon University, Pittsburgh, PA, USA\"\n",
    "\t\t\t\t],\n",
    "\t\t\t\t\"researcher_id\": \"ur.012471523754.61\"\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t\t\"date\": \"2022-12-31\",\n",
    "\t\t\"doi\": \"10.1145/3526104\",\n",
    "\t\t\"id\": \"pub.1147121755\",\n",
    "\t\t\"title\": \"The Impact of Route Descriptions on Human Expectations for Robot Navigation\",\n",
    "\t\t\"year\": 2022\n",
    "\t},\n",
    "\t{\n",
    "\t\t\"abstract\": \"Data-driven network intrusion detection (NID) has a tendency towards minority attack classes compared to normal traffic. Many datasets are collected in simulated environments rather than real-world networks. These challenges undermine the performance of intrusion detection machine learning models by fitting machine learning models to unrepresentative “sandbox” datasets. This survey presents a taxonomy with eight main challenges and explores common datasets from 1999 to 2020. Trends are analyzed on the challenges in the past decade and future directions are proposed on expanding NID into cloud-based environments, devising scalable models for large network data, and creating labeled datasets collected in real-world networks.\",\n",
    "\t\t\"authors\": [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"affiliations\": [\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"city\": \"Pittsburgh\",\n",
    "\t\t\t\t\t\t\"city_id\": 5206379,\n",
    "\t\t\t\t\t\t\"country\": \"United States\",\n",
    "\t\t\t\t\t\t\"country_code\": \"US\",\n",
    "\t\t\t\t\t\t\"id\": \"grid.147455.6\",\n",
    "\t\t\t\t\t\t\"name\": \"Carnegie Mellon University\",\n",
    "\t\t\t\t\t\t\"raw_affiliation\": \"Carnegie Mellon University, Pittsburgh, PA\",\n",
    "\t\t\t\t\t\t\"state\": \"Pennsylvania\",\n",
    "\t\t\t\t\t\t\"state_code\": \"US-PA\"\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t],\n",
    "\t\t\t\t\"corresponding\": \"\",\n",
    "\t\t\t\t\"current_organization_id\": \"\",\n",
    "\t\t\t\t\"first_name\": \"Dylan\",\n",
    "\t\t\t\t\"last_name\": \"Chou\",\n",
    "\t\t\t\t\"orcid\": [],\n",
    "\t\t\t\t\"raw_affiliation\": [\n",
    "\t\t\t\t\t\"Carnegie Mellon University, Pittsburgh, PA\"\n",
    "\t\t\t\t],\n",
    "\t\t\t\t\"researcher_id\": \"null\"\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"affiliations\": [\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"city\": \"Notre Dame\",\n",
    "\t\t\t\t\t\t\"city_id\": 8469294,\n",
    "\t\t\t\t\t\t\"country\": \"United States\",\n",
    "\t\t\t\t\t\t\"country_code\": \"US\",\n",
    "\t\t\t\t\t\t\"id\": \"grid.131063.6\",\n",
    "\t\t\t\t\t\t\"name\": \"University of Notre Dame\",\n",
    "\t\t\t\t\t\t\"raw_affiliation\": \"University of Notre Dame, Notre Dame, Indiana\",\n",
    "\t\t\t\t\t\t\"state\": \"Indiana\",\n",
    "\t\t\t\t\t\t\"state_code\": \"US-IN\"\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t],\n",
    "\t\t\t\t\"corresponding\": \"\",\n",
    "\t\t\t\t\"current_organization_id\": \"grid.131063.6\",\n",
    "\t\t\t\t\"first_name\": \"Meng\",\n",
    "\t\t\t\t\"last_name\": \"Jiang\",\n",
    "\t\t\t\t\"orcid\": [\n",
    "\t\t\t\t\t\"0000-0002-3009-519X\"\n",
    "\t\t\t\t],\n",
    "\t\t\t\t\"raw_affiliation\": [\n",
    "\t\t\t\t\t\"University of Notre Dame, Notre Dame, Indiana\"\n",
    "\t\t\t\t],\n",
    "\t\t\t\t\"researcher_id\": \"ur.010456457135.66\"\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t\t\"date\": \"2022-12-31\",\n",
    "\t\t\"doi\": \"10.1145/3472753\",\n",
    "\t\t\"id\": \"pub.1141731091\",\n",
    "\t\t\"title\": \"A Survey on Data-driven Network Intrusion Detection\",\n",
    "\t\t\"year\": 2022\n",
    "\t}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-biotechnology",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "jsonfile[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-heritage",
   "metadata": {},
   "source": [
    "## Format for upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pleased-recognition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 records are ready for upload.\n"
     ]
    }
   ],
   "source": [
    "#Format records for upload. Customize the Custom field section for your group.\n",
    "\n",
    "result = []\n",
    "doi_list = []\n",
    "for item in jsonfile:\n",
    "    my_dict={}\n",
    "    my_dict['title']=item.get('title')\n",
    "    if 'abstract' in item: #abstract isn't always present\n",
    "        my_dict['description']=item.get('abstract')\n",
    "    else:\n",
    "        my_dict['description']=\"No description available\"\n",
    "    authors = [] #format authors\n",
    "    for name in item['authors']:\n",
    "        authorname = {\"name\" : name['first_name'] + \" \" + name['last_name']}\n",
    "        authors.append(authorname)\n",
    "    my_dict['authors']= authors\n",
    "    my_dict['defined_type'] = 'journal contribution'\n",
    "    my_dict['doi']= item.get('doi')\n",
    "    my_dict['resource_doi']= item.get('doi')\n",
    "    my_dict['resource_title']=item.get('title')\n",
    "    #my_dict['references'] = [item.get('URL')]\n",
    "    my_dict['timeline'] =  {\"firstOnline\" : str(item['year']) + \"-01-01\"} #year only \n",
    "    #my_dict['is_metadata_record'] = True #Use these if you want a metadata only record\n",
    "    #my_dict['metadata_reason'] = 'See publisher version'\n",
    "    result.append(my_dict)\n",
    "    doi_list.append(item['doi'])\n",
    "\n",
    "\n",
    "print(len(result),\"records are ready for upload.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-tissue",
   "metadata": {},
   "source": [
    "## Validate metadata - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "related-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = json.dumps(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "violent-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonschema import validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eight-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I copied the schema from https://github.com/figshare/user_documentation/tree/master/swagger_documentation/documentation/models\n",
    "#and added the $id and $schema info\n",
    "base = json.loads(open('create-item.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abstract-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If there is no output, the validation is successful\n",
    "validate(test, schema=base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-better",
   "metadata": {},
   "source": [
    "## Upload the records with a link as the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "generous-supervisor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 records created and updated. There were 0 records that were not created at all.\n",
      "There were 0 records created and partially updated with authors (DOI failed).\n",
      "Failed record descriptions: []\n"
     ]
    }
   ],
   "source": [
    "#Upload the records\n",
    "\n",
    "record_fails = []\n",
    "partial_record_ids = []\n",
    "created_record_ids = [] #Use this to delete all the draft records if needed \n",
    "success_count = 0\n",
    "count = 0 #This just tracks what index value the loop is on and is used to connect the metadata with the DOI update\n",
    "\n",
    "for index, item in enumerate(result):\n",
    "    jsonresult = json.dumps(item) #Takes one record and makes it a json string (double quotes)\n",
    "    r = requests.post(BASE_URL + '/account/articles', headers=api_call_headers, data = jsonresult)\n",
    "    if r.status_code != 201:\n",
    "        record_fails.append(str(index) + \":\" + str(r.content[0:75])) #Add failed index to list with partial description\n",
    "        count += 1\n",
    "    else:\n",
    "        count += 1 #increment here otherwise have to do it at each if statement below\n",
    "        #Remove the admin account as an author by updating the record just created\n",
    "        #This uses the article url returned by the API response (r)\n",
    "        \n",
    "        # Get the location URL and item id\n",
    "        response_json = json.loads(r.content)\n",
    "        new_url = response_json['location']\n",
    "        item_id = response_json['entity_id']\n",
    "        created_record_ids.append(item_id)\n",
    "        \n",
    "        \n",
    "        #Format and update authors\n",
    "        authordict = {}\n",
    "        authordict['authors'] = item['authors']\n",
    "        authorjson = json.dumps(authordict) #formats everything with double quotes\n",
    "        s = requests.put(new_url, headers=api_call_headers, data = authorjson) \n",
    "        if r.status_code != 201:\n",
    "            record_fails.append(str(index) + \"failed at author update:\" + str(r.content[0:75])) #Add failed index to list with partial description         \n",
    "            partial_record_ids.append(item_id)\n",
    "        else:\n",
    "            #Upload a link as a file\n",
    "            link = '{\"link\":\"https://doi.org/'+ str(doi_list[count-1]) + '\"}' #count-1 because already incremented value to next index\n",
    "            t = requests.post(new_url +'/files', headers=api_call_headers, data = link)\n",
    "            if r.status_code != 201:\n",
    "                record_fails.append(str(index) + \"failed at doi update:\" + str(r.content[0:75])) #Add failed index to list with partial description\n",
    "                partial_record_ids.append(item_id)\n",
    "            else:\n",
    "                success_count += 1\n",
    "\n",
    "        \n",
    "print(success_count,\"records created and updated. There were\",len(result)-len(created_record_ids),\"records that were not created at all.\")\n",
    "print('There were',len(partial_record_ids),'records created but with a failed DOI link.')\n",
    "print(\"Failed record descriptions:\",record_fails)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-westminster",
   "metadata": {},
   "source": [
    "## Note: This does not add categories or keywords, which are required for publishing. Could add those as part of this process, and then publish.\n",
    "\n",
    "If you have review on and if you do upload all metadata, the best way to publish all at once is to download the metadata through the batch management tool and then reupload that CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-communist",
   "metadata": {},
   "source": [
    "## For testing purposes, use this to delete all the records you just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "technical-netscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record deleted\n",
      "Record deleted\n"
     ]
    }
   ],
   "source": [
    "delete_record_fails = []\n",
    "\n",
    "for item in created_record_ids:\n",
    "    r = requests.delete(BASE_URL + '/account/articles/' + str(item), headers=api_call_headers)\n",
    "    if r.status_code != 204:\n",
    "        delete_record_fails.append(str(index) + \":\" + str(r.content[0:75])) #Add failed index to list with partial description\n",
    "    else:\n",
    "        print(\"Record deleted\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-crash",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
