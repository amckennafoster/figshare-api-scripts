{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "thorough-dictionary",
   "metadata": {},
   "source": [
    "## Collect all item metadata and output tables linked by item id\n",
    "\n",
    "Tables: metadata, authors, funding, tags, categories, custom fields\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-williams",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e37a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-lending",
   "metadata": {},
   "source": [
    "## Set token, admin id, and base URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "continued-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the token in the header.\n",
    "text_file = open(\"././././testing-token.txt\", \"r\") #Paste your token in a text file and save it where this notebook is\n",
    "TOKEN = text_file.read()\n",
    "TOKEN.strip() #removes any hidden spaces\n",
    "text_file.close()\n",
    "\n",
    "\n",
    "api_call_headers = {'Authorization': 'token ' + TOKEN}\n",
    "\n",
    "#Set the base URL\n",
    "BASE_URL = 'https://api.figsh.com/v2' #Change this if you want the production environment\n",
    "\n",
    "#Set file name descriptor\n",
    "descriptor = 'metadata'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-blowing",
   "metadata": {},
   "source": [
    "## Retrieve Metadata\n",
    "1. Get basic metadata by impersonating accounts (need a list of account ids with the items they own)\n",
    "2. Get a list of all public metadata and get a list of ids\n",
    "3. Subtract public articles from the list of private metadata ids (separating only draft items or fully embargoed items)\n",
    "4. Use your choice of list of item ids to retrieve all metadata fields for each article\n",
    "5. Convert the resulting JSON to a dataframe\n",
    "6. Save the dataframe to CSV or Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mental-toddler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 metadata records collected\n"
     ]
    }
   ],
   "source": [
    "private_items = []\n",
    "for i in range(1,2):\n",
    "    item = json.loads(requests.get(BASE_URL + '/account/institution/articles?page_size=1000&page={}'.format(i), headers=api_call_headers).content)\n",
    "    private_items.extend(item)\n",
    "print(len(private_items),'metadata records collected')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "second-absolute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 records kept, 22 records removed\n"
     ]
    }
   ],
   "source": [
    "#Keep records that are either public or fully embargoed, i.e. not drafts of never published records\n",
    "published_items = []\n",
    "for item in private_items:\n",
    "    if item['published_date'] != None: #if a record has a published date\n",
    "           published_items.append(item)\n",
    "            \n",
    "print(len(published_items), \"records kept,\",len(private_items) - len(published_items),\"records removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "respiratory-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the json\n",
    "with open('published_items-'+str(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.json', 'w') as f:\n",
    "    json.dump(published_items, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-scroll",
   "metadata": {},
   "source": [
    "## Collect metadata and views for each item\n",
    "This also sets up dictionaries for certain metadata elements that will become their own tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_metadata = []\n",
    "author_metadata = []\n",
    "funding_metadata = []\n",
    "categories_metadata = []\n",
    "tags_data = []\n",
    "files_metadata = []\n",
    "unpublished_count = 0\n",
    "\n",
    "for item in published_items:\n",
    "    m = requests.get(BASE_URL + '/articles/' + str(item['id']), headers=api_call_headers)\n",
    "    metadata=json.loads(m.text)\n",
    "    \n",
    "    if m.status_code == 200: #if the record is not a previously published and then unpublished record\n",
    "\n",
    "        views = json.loads(requests.get('https://stats.figshare.com/total/views/article/' + str(item['id']), headers=api_call_headers).content)\n",
    "        #Add views to the full metadata record and add to the main list\n",
    "        metadata['views'] = views['totals']\n",
    "        item_metadata.append(metadata)\n",
    "\n",
    "        #Add item id to each set of content for individual tables\n",
    "        authors = metadata['authors']\n",
    "        for a in authors:\n",
    "            a['item_id'] = item['id']\n",
    "            author_metadata.append(a)\n",
    "\n",
    "        funding = metadata['funding_list']\n",
    "        for f in funding:\n",
    "            f['item_id'] = item['id']\n",
    "            funding_metadata.append(f)\n",
    "    \n",
    "        cats = metadata['categories']\n",
    "        for c in cats:\n",
    "            c['item_id'] = item['id']\n",
    "            categories_metadata.append(c)\n",
    "    \n",
    "        for t in metadata['tags']: #tags are a list so its a bit different\n",
    "            tags = {}\n",
    "            tags['item_id'] = item['id']\n",
    "            tags['name'] = t\n",
    "            tags_data.append(tags)\n",
    "\n",
    "        if metadata['is_embargoed'] == 0: #If the record is not embargoed\n",
    "            if len(metadata['files']) > 0: #If the record is not 'metadata only'\n",
    "                files = metadata['files']\n",
    "                for f in files:\n",
    "                    f['item_id'] = item['id']\n",
    "                    files_metadata.append(f)\n",
    "    else:\n",
    "        unpublished_count += 1\n",
    "    \n",
    "    \n",
    "print('Full metadata for',len(item_metadata),'items retrieved.', unpublished_count,'items had a published date but are drafts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "extreme-vegetarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full metadata for 70 items retrieved. 18 items had a published date but are drafts\n"
     ]
    }
   ],
   "source": [
    "print('Full metadata for',len(item_metadata),'items retrieved.', unpublished_count,'items had a published date but are drafts')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-great",
   "metadata": {},
   "source": [
    "### Create separate tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "systematic-slovenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "authordf = pd.json_normalize(author_metadata)\n",
    "save_file = authordf.to_csv(descriptor + '-authors.csv',encoding='utf-8')\n",
    "\n",
    "funderdf = pd.json_normalize(funding_metadata)\n",
    "save_file = funderdf.to_csv(descriptor + '-funding.csv',encoding='utf-8')\n",
    "\n",
    "categorydf = pd.json_normalize(categories_metadata)\n",
    "save_file = categorydf.to_csv(descriptor + '-categories.csv',encoding='utf-8')\n",
    "\n",
    "tagdf = pd.DataFrame(tags_data, columns=[\"item_id\",\"name\"], index=None)\n",
    "\n",
    "filesdf = pd.json_normalize(files_metadata)\n",
    "save_file = filesdf.to_csv(descriptor + '-files.csv',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "beneficial-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the json. Change the file name to represent the list of ids you used.\n",
    "with open('full_records-'+str(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.json', 'w') as f:\n",
    "    json.dump(item_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2149a183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe from the JSON formatted data\n",
    "df = pd.DataFrame(item_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-valentine",
   "metadata": {},
   "source": [
    "## Format the spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-earthquake",
   "metadata": {},
   "source": [
    "### Remove the columns that are now separate tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dried-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['files', 'authors','funding_list','tags','categories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-recipient",
   "metadata": {},
   "source": [
    "### Split out the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "thick-preliminary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates split out and merged\n"
     ]
    }
   ],
   "source": [
    "#The dates are all contained within one column called 'timeline'. Flatten that column and associate the values\n",
    "#with the proper article id in a new dataframe\n",
    "\n",
    "temp_date_list = []\n",
    "\n",
    "for item in item_metadata:\n",
    "    dateitem = item['timeline']\n",
    "    dateitem['id'] = item['id']\n",
    "    temp_date_list.append(dateitem)\n",
    "\n",
    "df_dates = pd.json_normalize(\n",
    "    temp_date_list \n",
    ")\n",
    "\n",
    "#Merge the dataframes\n",
    "df_formatted = df.merge(df_dates, how='outer', on='id')\n",
    "\n",
    "print(\"Dates split out and merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-penalty",
   "metadata": {},
   "source": [
    "### Add Group names\n",
    "This retrieves a list of Groups and then formats the dataframe so that each group has id of its parent Group. The top level group has itself as the parent. The group names are then added to the main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bored-genius",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names for 11 different groups were added to the metadata records\n"
     ]
    }
   ],
   "source": [
    "#Get list of groups. \n",
    "s=requests.get(BASE_URL + '/account/institution/groups', headers=api_call_headers)\n",
    "groups=json.loads(s.text)\n",
    "\n",
    "#Create a dataframe of groups\n",
    "df_groups = pd.json_normalize(groups)\n",
    "\n",
    "df_groups_parent = df_groups[['id','name']] #Create reference dataframe\n",
    "df_groups = df_groups.rename(columns={'id': 'group_id','name': 'group_name'}) #Rename id col in main dataframe\n",
    "df_groups_parent = df_groups_parent.rename(columns={'name': 'parent_group_name'}) #Rename name col in reference dataframe\n",
    "\n",
    "df_groups = df_groups.sort_values(by=['parent_id'])\n",
    "top_group_id = df_groups.iloc[0]['group_id'] #Store the group id for top group \n",
    "\n",
    "df_groups.loc[df_groups['parent_id'] == 0, 'parent_id'] = top_group_id #For top level group, replace the zero value parent id with top level group id\n",
    "\n",
    "df_groups = df_groups.merge(df_groups_parent, how='inner',left_on=['parent_id'], right_on=['id']) #Add parent group name\n",
    "\n",
    "df_groups = df_groups[['group_id','group_name','parent_group_name']] #Pare down to needed columns\n",
    "\n",
    "\n",
    "#Merge the dataframes \n",
    "df_formatted = df_formatted.merge(df_groups, how='inner', on='group_id') #If you use 'outer' it will include a blank record for each group with no records\n",
    "\n",
    "print(\"Names for\",len(df_groups),\"different groups were added to the metadata records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-nation",
   "metadata": {},
   "source": [
    "### Split out custom fields\n",
    "This creates new columns for each custom field.\n",
    "\n",
    "If different groups have different custom metadata, check the output carefully to make sure things mapped properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "alleged-cartoon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom fields split out and merged\n"
     ]
    }
   ],
   "source": [
    "#The custom fields are all contained within one column called 'custom_fields'. Flatten that column and associate the values\n",
    "#with the proper article id in a new dataframe\n",
    "custom = pd.json_normalize(\n",
    "    item_metadata, \n",
    "    record_path =['custom_fields'], \n",
    "    meta=['id']\n",
    ")\n",
    "\n",
    "if len(custom) > 0:\n",
    "    #This reshapes the data so that metadata field names are columns and each row is an id.\n",
    "    custom = custom.pivot(index=\"id\", columns=\"name\", values=\"value\")\n",
    "    \n",
    "    #Merge the dataframes so that all the custom fields are visible along with all the other metadata\n",
    "    df_formatted = df_formatted.merge(custom, how='outer', on='id') #Outer merge keeps records that have no custom metadata.\n",
    "    print('Custom fields split out and merged')\n",
    "else:\n",
    "    print('No custom fields to split out')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-adoption",
   "metadata": {},
   "source": [
    "## Save the spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "robust-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a CSV file of all the metadata. Change the file name if necessary to match dates.\n",
    "save_file = df_formatted.to_csv(descriptor + '-main.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "composite-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or save an Excel file of all the metadata. Change the file name if necessary to match dates.\n",
    "#save_file = df_formatted.to_excel('all-records-'+str(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
