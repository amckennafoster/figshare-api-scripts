{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "thorough-dictionary",
   "metadata": {},
   "source": [
    "## Collect all item metadata and output tables linked by item id\n",
    "\n",
    "Tables: metadata, authors, funding, tags, categories, custom fields\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-williams",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e37a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-lending",
   "metadata": {},
   "source": [
    "## Set token, admin id, and base URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "continued-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the token in the header.\n",
    "text_file = open(\"team-token.txt\", \"r\") #Paste your token in a text file and save it where this notebook is\n",
    "TOKEN = text_file.read()\n",
    "TOKEN.strip() #removes any hidden spaces\n",
    "text_file.close()\n",
    "\n",
    "\n",
    "api_call_headers = {'Authorization': 'token ' + TOKEN}\n",
    "\n",
    "#Set the base URL\n",
    "BASE_URL = 'https://api.figshare.com/v2' #Change this if you want the production environment\n",
    "\n",
    "#Set file name descriptor\n",
    "descriptor = 'metadata'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-blowing",
   "metadata": {},
   "source": [
    "## Retrieve Metadata\n",
    "1. Get basic metadata by impersonating accounts (need a list of account ids with the items they own)\n",
    "2. Get a list of all public metadata and get a list of ids\n",
    "3. Subtract public articles from the list of private metadata ids (separating only draft items or fully embargoed items)\n",
    "4. Use your choice of list of item ids to retrieve all metadata fields for each article\n",
    "5. Convert the resulting JSON to a dataframe\n",
    "6. Save the dataframe to CSV or Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mental-toddler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194 metadata records collected\n"
     ]
    }
   ],
   "source": [
    "private_items = []\n",
    "for i in range(1,2):\n",
    "    item = json.loads(requests.get(BASE_URL + '/account/institution/articles?page_size=1000&page={}'.format(i), headers=api_call_headers).content)\n",
    "    private_items.extend(item)\n",
    "print(len(private_items),'metadata records collected')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "second-absolute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 records kept, 157 records removed\n"
     ]
    }
   ],
   "source": [
    "#Keep records that are either public or fully embargoed, i.e. not drafts of never published records\n",
    "published_items = []\n",
    "for item in private_items:\n",
    "    if item['published_date'] != None: #if a record has a published date\n",
    "           published_items.append(item)\n",
    "            \n",
    "print(len(published_items), \"records kept,\",len(private_items) - len(published_items),\"records removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "respiratory-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the json\n",
    "with open('published_items-'+str(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.json', 'w') as f:\n",
    "    json.dump(published_items, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-scroll",
   "metadata": {},
   "source": [
    "## Collect metadata and views for each item\n",
    "This also sets up dictionaries for certain metadata elements that will become their own tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "charming-steal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full metadata for 34 items retrieved. 3 items had a published date but are drafts\n"
     ]
    }
   ],
   "source": [
    "item_metadata = []\n",
    "author_metadata = []\n",
    "funding_metadata = []\n",
    "categories_metadata = []\n",
    "tags_data = []\n",
    "files_metadata = []\n",
    "unpublished_count = 0\n",
    "\n",
    "for item in published_items:\n",
    "    m = requests.get(BASE_URL + '/articles/' + str(item['id']), headers=api_call_headers)\n",
    "    metadata=json.loads(m.text)\n",
    "    \n",
    "    if m.status_code == 200: #if the record is not a previously published and then unpublished record\n",
    "\n",
    "        views = json.loads(requests.get('https://stats.figshare.com/total/views/article/' + str(item['id']), headers=api_call_headers).content)\n",
    "        #Add views to the full metadata record and add to the main list\n",
    "        metadata['views'] = views['totals']\n",
    "        item_metadata.append(metadata)\n",
    "\n",
    "        #Add item id to each set of content for individual tables\n",
    "        authors = metadata['authors']\n",
    "        for a in authors:\n",
    "            a['item_id'] = item['id']\n",
    "            author_metadata.append(a)\n",
    "\n",
    "        funding = metadata['funding_list']\n",
    "        for f in funding:\n",
    "            f['item_id'] = item['id']\n",
    "            funding_metadata.append(f)\n",
    "    \n",
    "        cats = metadata['categories']\n",
    "        for c in cats:\n",
    "            c['item_id'] = item['id']\n",
    "            categories_metadata.append(c)\n",
    "    \n",
    "        for t in metadata['tags']: #tags are a list so its a bit different\n",
    "            tags = {}\n",
    "            tags['item_id'] = item['id']\n",
    "            tags['name'] = t\n",
    "            tags_data.append(tags)\n",
    "\n",
    "        if metadata['is_embargoed'] == 0: #If the record is not embargoed\n",
    "            if len(metadata['files']) > 0: #If the record is not 'metadata only'\n",
    "                files = metadata['files']\n",
    "                for f in files:\n",
    "                    f['item_id'] = item['id']\n",
    "                    files_metadata.append(f)\n",
    "    else:\n",
    "        unpublished_count += 1\n",
    "    \n",
    "    \n",
    "print('Full metadata for',len(item_metadata),'items retrieved.', unpublished_count,'items had a published date but are drafts')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-great",
   "metadata": {},
   "source": [
    "### Create separate tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "beneficial-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the json. Change the file name to represent the list of ids you used.\n",
    "with open('full_records-'+str(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.json', 'w') as f:\n",
    "    json.dump(item_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2149a183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Create a dataframe from the JSON formatted data\n",
    "df = pd.DataFrame(item_metadata)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-valentine",
   "metadata": {},
   "source": [
    "## Format the spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-recipient",
   "metadata": {},
   "source": [
    "### Split out the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "thick-preliminary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates split out and merged\n"
     ]
    }
   ],
   "source": [
    "#The dates are all contained within one column called 'timeline'. Flatten that column and associate the values\n",
    "#with the proper article id in a new dataframe\n",
    "\n",
    "temp_date_list = []\n",
    "\n",
    "for item in item_metadata:\n",
    "    dateitem = item['timeline']\n",
    "    dateitem['id'] = item['id']\n",
    "    temp_date_list.append(dateitem)\n",
    "\n",
    "df_dates = pd.json_normalize(\n",
    "    temp_date_list \n",
    ")\n",
    "\n",
    "df_dates.replace(df_dates.replace(r'^\\s*$', 'null', regex=True, inplace = True)) #Replace blank cells with 'null'\n",
    "\n",
    "#Merge the dataframes\n",
    "df_formatted = df.merge(df_dates, how='outer', on='id')\n",
    "\n",
    "print(\"Dates split out and merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-penalty",
   "metadata": {},
   "source": [
    "### Add Group names\n",
    "This retrieves a list of Groups and then formats the dataframe so that each group has id of its parent Group. The top level group has itself as the parent. The group names are then added to the main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bored-genius",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names for 5 different groups were added to the metadata records\n"
     ]
    }
   ],
   "source": [
    "#Get list of groups. \n",
    "s=requests.get(BASE_URL + '/account/institution/groups', headers=api_call_headers)\n",
    "groups=json.loads(s.text)\n",
    "\n",
    "#Create a dataframe of groups\n",
    "df_groups = pd.json_normalize(groups)\n",
    "\n",
    "df_groups_parent = df_groups[['id','name']] #Create reference dataframe\n",
    "df_groups = df_groups.rename(columns={'id': 'group_id','name': 'group_name'}) #Rename id col in main dataframe\n",
    "df_groups_parent = df_groups_parent.rename(columns={'name': 'parent_group_name'}) #Rename name col in reference dataframe\n",
    "\n",
    "df_groups = df_groups.sort_values(by=['parent_id'])\n",
    "top_group_id = df_groups.iloc[0]['group_id'] #Store the group id for top group \n",
    "\n",
    "df_groups.loc[df_groups['parent_id'] == 0, 'parent_id'] = top_group_id #For top level group, replace the zero value parent id with top level group id\n",
    "\n",
    "df_groups = df_groups.merge(df_groups_parent, how='inner',left_on=['parent_id'], right_on=['id']) #Add parent group name\n",
    "\n",
    "df_groups = df_groups[['group_id','group_name','parent_group_name']] #Pare down to needed columns\n",
    "\n",
    "\n",
    "#Merge the dataframes \n",
    "df_formatted = df_formatted.merge(df_groups, how='inner', on='group_id') #If you use 'outer' it will include a blank record for each group with no records\n",
    "\n",
    "print(\"Names for\",len(df_groups),\"different groups were added to the metadata records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-nation",
   "metadata": {},
   "source": [
    "### Split out custom fields\n",
    "This creates new columns for each custom field.\n",
    "\n",
    "If different groups have different custom metadata, check the output carefully to make sure things mapped properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "alleged-cartoon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom fields split out and merged\n"
     ]
    }
   ],
   "source": [
    "#The custom fields are all contained within one column called 'custom_fields'. Flatten that column and associate the values\n",
    "#with the proper article id in a new dataframe\n",
    "custom = pd.json_normalize(\n",
    "    item_metadata, \n",
    "    record_path =['custom_fields'], \n",
    "    meta=['id']\n",
    ")\n",
    "\n",
    "if len(custom) > 0:\n",
    "    #This reshapes the data so that metadata field names are columns and each row is an id.\n",
    "    custom = custom.pivot(index=\"id\", columns=\"name\", values=\"value\")\n",
    "    \n",
    "    #Merge the dataframes so that all the custom fields are visible along with all the other metadata\n",
    "    df_formatted = df_formatted.merge(custom, how='outer', on='id') #Outer merge keeps records that have no custom metadata.\n",
    "    print('Custom fields split out and merged')\n",
    "else:\n",
    "    print('No custom fields to split out')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-earthquake",
   "metadata": {},
   "source": [
    "### Remove the columns that are now separate tables or are slit to new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dried-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted = df_formatted.drop(columns=['files', 'authors','funding','funding_list','tags','categories','timeline','custom_fields'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-street",
   "metadata": {},
   "source": [
    "### Fill in blank cells and rename the id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "elementary-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted.replace(df_formatted.replace(r'^\\s*$', 'null', regex=True, inplace = True)) #Replace blank cells with 'null'\n",
    "#df_formatted.embargo_date.fillna(value='null', inplace=True)\n",
    "df_formatted.fillna(value='null', inplace=True)\n",
    "df_formatted = df_formatted.rename(columns={'id': 'item_id'}) #Rename id col in main dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-chinese",
   "metadata": {},
   "source": [
    "## Create all the other dataframes for saving later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "authordf = pd.json_normalize(author_metadata)\n",
    "authordf.replace(authordf.replace(r'^\\s*$', 'null', regex=True, inplace = True)) #Replace blank cells with 'null'\n",
    "\n",
    "funderdf = pd.json_normalize(funding_metadata)\n",
    "funderdf.replace(funderdf.replace(r'^\\s*$', 'null', regex=True, inplace = True)) #Replace blank cells with 'null'\n",
    "\n",
    "categorydf = pd.json_normalize(categories_metadata)\n",
    "\n",
    "tagdf = pd.DataFrame(tags_data, columns=[\"item_id\",\"name\"], index=None)\n",
    "\n",
    "filesdf = pd.json_normalize(files_metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-ticket",
   "metadata": {},
   "source": [
    "## Get some other stats\n",
    "This requires separate credentials that you need to request from Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "explicit-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "perceived-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the institution string, Basic authorization string, and base URL\n",
    "\n",
    "INST = 'team'\n",
    "\n",
    "text_file = open(\"team-stats-token.txt\", \"r\")\n",
    "auth = text_file.read()\n",
    "auth.strip() #removes any hidden spaces\n",
    "text_file.close()\n",
    "\n",
    "#Set the base URL\n",
    "BASE_URL2 = 'https://stats.figshare.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "corresponding-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the username and password \n",
    "sample_string = auth\n",
    "sample_string_bytes = sample_string.encode(\"ascii\")\n",
    "\n",
    "base64_bytes = base64.b64encode(sample_string_bytes)\n",
    "base64_string = base64_bytes.decode(\"ascii\")\n",
    "\n",
    "api_call_headers2 = {'Authorization': 'Basic ' + base64_string}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "identified-columbia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "#List of public datasets:\n",
    "item_ids = df_formatted['item_id'].values.tolist()\n",
    "print(len(item_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-finance",
   "metadata": {},
   "source": [
    "### Get views over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "streaming-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the views each month\n",
    "df_timeline = pd.DataFrame(columns=['item_id','index',0]) #Set up a dataframe to accept values\n",
    "\n",
    "for item in item_ids:\n",
    "    URL = BASE_URL2 + INST + '/timeline/month/views/article/' + str(item)# + '?start_date=2018-01-01&end_date=2023-05-01'\n",
    "    r = requests.get(URL, headers=api_call_headers2)\n",
    "    response = r.content.decode('utf-8')\n",
    "    response_dict = json.loads(response)\n",
    "    if response_dict['timeline'] != None: #If there are some views to record\n",
    "        #Format the JSON in the response\n",
    "        df1 = pd.json_normalize(response_dict['timeline'])\n",
    "        df = df1.T\n",
    "        df.reset_index(inplace=True)\n",
    "        df['item_id'] = item['id']\n",
    "        df_timeline = df_timeline.append(df) #append data to the dataframe\n",
    "    \n",
    "#Rename columns \n",
    "df_timeline = df_timeline.rename(columns = {'index':'date', 0:'value'})    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "prescribed-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = df_timeline.to_csv('timeline.csv',encoding='utf-8', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-apparatus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-flash",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "perfect-width",
   "metadata": {},
   "source": [
    "### Get views by geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of group ids\n",
    "r = requests.get(BASE_URL + '/account/institution/groups', headers=api_call_headers)\n",
    "result=json.loads(r.text)\n",
    "groups = []\n",
    "for i in result:\n",
    "    groups.append({'grp_id':i['id'],'grp_name':i['name']})\n",
    "\n",
    "print('Group ids for', len(groups), 'groups collected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect views by group by geolocation for the last month\n",
    "breakdown = []\n",
    "for g in groups:\n",
    "    URL = BASE_URL2 + '/' + INST + '/breakdown/total/views/group/' + str(g['grp_id']) #by not specifying timeframe, will retrieve for the last month\n",
    "    r = requests.get(URL, headers=api_call_headers2)\n",
    "    result=json.loads(r.text)\n",
    "    result['group'] = g['grp_name']\n",
    "    breakdown.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The results may have nested values for cities. Just get the totals by country and create a dataframe\n",
    "country_totals = []\n",
    "for record in breakdown:\n",
    "    for item in record['breakdown']['total']:\n",
    "        country_totals.append({'group':record['group'],'country':item,'total':record['breakdown']['total'][item]['total']})\n",
    "df_geoviews = pd.DataFrame(country_totals)\n",
    "df_geoviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-neutral",
   "metadata": {},
   "source": [
    "## Save all dataframes to one Excel workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "thermal-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"metadata.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index\n",
    "    # to store the dataframe in specified sheet\n",
    "    df_formatted.to_excel(writer, sheet_name=\"main\", index=False)\n",
    "    authordf.to_excel(writer, sheet_name=\"authors\", index=False)\n",
    "    funderdf.to_excel(writer, sheet_name=\"funding\", index=False)\n",
    "    categorydf.to_excel(writer, sheet_name=\"categories\", index=False)\n",
    "    tagdf.to_excel(writer, sheet_name=\"tags\", index=False)\n",
    "    filesdf.to_excel(writer, sheet_name=\"files\", index=False)\n",
    "    df_timeline.to_excel(writer, sheet_name=\"timeline\", index=False)\n",
    "    df_geoviews.to_excel(writer, sheet_name=\"geoviews\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-adoption",
   "metadata": {},
   "source": [
    "## Save to separate CSVs if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "systematic-slovenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_file = authordf.to_csv(descriptor + '-authors.csv',encoding='utf-8', index=None)\n",
    "\n",
    "save_file = funderdf.to_csv(descriptor + '-funding.csv',encoding='utf-8', index=None)\n",
    "\n",
    "save_file = categorydf.to_csv(descriptor + '-categories.csv',encoding='utf-8', index=None)\n",
    "\n",
    "save_file = tagdf.to_csv(descriptor + '-tags.csv',encoding='utf-8', index=None)\n",
    "\n",
    "save_file = filesdf.to_csv(descriptor + '-files.csv',encoding='utf-8', index=None)\n",
    "\n",
    "save_file = df_formatted.to_csv(descriptor + '-main.csv',encoding='utf-8', index=None)\n",
    "\n",
    "save_file = df_timeline.to_csv(descriptor + '-timeline.csv',encoding='utf-8', index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-square",
   "metadata": {},
   "source": [
    "## Reopen files if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pending-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you need to open up all the CSVs again\n",
    "authordf = pd.read_csv('metadata-authors.csv')\n",
    "funderdf = pd.read_csv('metadata-funding.csv')\n",
    "categorydf = pd.read_csv('metadata-categories.csv')\n",
    "tagdf = pd.read_csv('metadata-tags.csv')\n",
    "filesdf = pd.read_csv('metadata-files.csv')\n",
    "df_formatted = pd.read_csv('metadata-main.csv')\n",
    "df_timeline = pd.read_csv('metadata-timeline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-cross",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-weather",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-zambia",
   "metadata": {},
   "source": [
    "## Adding CSVs to DataStudio\n",
    "\n",
    "All the CSVs except the main metadata one seem to upload fine. The main metadata one has to be converted to a google sheet for some reason\n",
    "\n",
    "Or you can upload the metadata.xlsx and connect each sheet within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "composite-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or save an Excel file of all the metadata. Change the file name if necessary to match dates.\n",
    "#save_file = df_formatted.to_excel('all-records-'+str(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
