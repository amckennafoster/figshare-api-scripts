{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "thorough-dictionary",
   "metadata": {},
   "source": [
    "## This retrieves all metadata including for private or fully embargoed items in all accounts\n",
    "For demonstration only and would need slight tweaking to give you exactly the metadata you want.\n",
    "This does not retrieve metadata for collections or projects.\n",
    "\n",
    "The end result is a spreadsheet of metadata with the several things added or modified:\n",
    "1. The item owners name and email is added\n",
    "2. The group the item belongs to is added\n",
    "3. The author names are formatted to be more readable and ORCID is included\n",
    "4. The dates are split out into their own columns\n",
    "5. Any custom fields are separated out into their own columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-williams",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e37a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-lending",
   "metadata": {},
   "source": [
    "## Set token, admin id, and base URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continued-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the token in the header.\n",
    "api_call_headers = {'Authorization': 'token ENTER TOKEN'} #example: {'Authorization': 'token dkd8rskjdkfiwi49hgkw...'}\n",
    "\n",
    "#Don't want to impersonate the admin account you are using so put that id here. Retrieve this from this \n",
    "#  endpoint (put the token in the upper left box): https://docs.figsh.com/#private_institution_accounts_list\n",
    "token_user_id = ENTER ID #example: token_user_id = 2938474\n",
    "\n",
    "#Set the base URL\n",
    "BASE_URL = 'https://api.figsh.com/v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-blowing",
   "metadata": {},
   "source": [
    "## Retrieve Metadata\n",
    "1. Get a list of basic metadata for all private records\n",
    "2. Select only published records (includes embargoed records)\n",
    "3. For each record get the full metadata and add in the owner name and email  \n",
    "4. Format dates\n",
    "5. Add in the name of the Group the record is part of\n",
    "6. Split out the custom metadata\n",
    "7. Save the dataframe to CSV or Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "international-arizona",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathered 42 private articles\n"
     ]
    }
   ],
   "source": [
    "#Gather all private records (make sure your token is for a top level admin account)\n",
    "private_records = []\n",
    "for i in range(1,2):\n",
    "    records = json.loads(requests.get(BASE_URL + '/account/institution/articles?page_size=1000&page={}'.format(i), headers=api_call_headers).content)\n",
    "    private_records.extend(records)\n",
    "\n",
    "print('Gathered',len(private_records),'private records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "signal-terrorist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 records kept, 10 records removed\n"
     ]
    }
   ],
   "source": [
    "#Keep records that are either public or fully embargoed\n",
    "published_records = []\n",
    "for item in private_records:\n",
    "    if item['published_date'] != None: #if a record has a published date\n",
    "           published_records.append(item)\n",
    "            \n",
    "print(len(published_records), \"records kept,\",len(private_records) - len(published_records),\"records removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-official",
   "metadata": {},
   "source": [
    "## Collect full metadata\n",
    "Using the list you have of basic metadata with owner id and owner name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "expanded-emphasis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full metadata for 32 records retrieved\n"
     ]
    }
   ],
   "source": [
    "#For each id in the list, retrieve all the metadata for the article by visiting the Figshare article API endpoint \n",
    "#Admin token does not need to impersonate\n",
    "#This may take a while if there are a lot of items. ~1.5 seconds per item\n",
    "\n",
    "#First get a list of all the users. Email and name will be extracted based on account id later\n",
    "users = []\n",
    "for i in range(1,2):\n",
    "    usr = json.loads(requests.get(BASE_URL + '/account/institution/accounts?page_size=1000&page={}'.format(i), headers=api_call_headers).content)\n",
    "    users.extend(usr)\n",
    "\n",
    "#Then gather and format each record:\n",
    "full_records = []\n",
    "for item in published_records: \n",
    "    s=requests.get(BASE_URL + '/account/articles/' + str(item['id']), headers=api_call_headers)\n",
    "    metadata=json.loads(s.text)\n",
    "    counter = 0\n",
    "    author_list = \"\"\n",
    "    author_count = len(metadata['authors'])\n",
    "    for name in metadata['authors']: #Format author list to be readable\n",
    "        if counter == 0:\n",
    "            author_list = author_list + name['full_name'] + ' (ORCID: ' + name['orcid_id'] + ')'\n",
    "            counter += 1\n",
    "        elif counter < author_count:\n",
    "            author_list = author_list + ' | ' + name['full_name'] + ' (ORCID: ' + name['orcid_id'] + ')'\n",
    "            counter += 1\n",
    "    metadata['author_readable'] = author_list\n",
    "    \n",
    "    for person in users:\n",
    "        if person['id'] == metadata['account_id']: \n",
    "            metadata['record_owner_name'] = person['first_name'] + ' ' + person['last_name'] #add in user name\n",
    "            metadata['owner_email'] = person['email'] #add user email\n",
    "\n",
    "    full_records.append(metadata)\n",
    "\n",
    "print('Full metadata for',len(full_records),'records retrieved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "beneficial-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL: save the json. Change the file name to represent the list of ids you used.\n",
    "with open('full_records-'+str(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.json', 'w') as f:\n",
    "    json.dump(full_records, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2149a183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe from the JSON formatted data\n",
    "df = pd.DataFrame(full_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-binary",
   "metadata": {},
   "source": [
    "### Open a previous json file if you need to, otherwise skip the the Formatting section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "amino-removal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 records\n"
     ]
    }
   ],
   "source": [
    "#If needed, open up the same file for reading. Replace the file titles as needed.\n",
    "with open(\"full_records-DATE.json\", \"r\", encoding='utf8') as read_file: #Replace this with the filename of your choice\n",
    "    full_articles = json.load(read_file)\n",
    "    \n",
    "#Create a dataframe from the JSON formatted data\n",
    "df = pd.DataFrame(full_records)\n",
    "\n",
    "print(len(full_records),\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-valentine",
   "metadata": {},
   "source": [
    "## Format the spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-recipient",
   "metadata": {},
   "source": [
    "### Split out the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "thick-preliminary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates split out and merged\n"
     ]
    }
   ],
   "source": [
    "#The dates are all contained within one column called 'timeline'. Flatten that column and associate the values\n",
    "#with the proper article id in a new dataframe\n",
    "\n",
    "temp_date_list = []\n",
    "\n",
    "for item in full_records:\n",
    "    dateitem = item['timeline']\n",
    "    dateitem['id'] = item['id']\n",
    "    temp_date_list.append(dateitem)\n",
    "\n",
    "df_dates = pd.json_normalize(\n",
    "    temp_date_list \n",
    ")\n",
    "\n",
    "#Merge the dataframes\n",
    "df_formatted = df.merge(df_dates, how='outer', on='id')\n",
    "\n",
    "print(\"Dates split out and merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-penalty",
   "metadata": {},
   "source": [
    "### Add Group names\n",
    "This retrieves a list of Groups and then formats the dataframe so that each group has id of its parent Group. The top level group has itself as the parent. The group names are then added to the main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bored-genius",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names for 11 different groups were added to the metadata records\n"
     ]
    }
   ],
   "source": [
    "#Get list of groups. \n",
    "s=requests.get(BASE_URL + '/account/institution/groups', headers=api_call_headers)\n",
    "groups=json.loads(s.text)\n",
    "\n",
    "#Create a dataframe of groups\n",
    "df_groups = pd.json_normalize(groups)\n",
    "\n",
    "df_groups_parent = df_groups[['id','name']] #Create reference dataframe\n",
    "df_groups = df_groups.rename(columns={'id': 'group_id','name': 'group_name'}) #Rename id col in main dataframe\n",
    "df_groups_parent = df_groups_parent.rename(columns={'name': 'parent_group_name'}) #Rename name col in reference dataframe\n",
    "\n",
    "df_groups = df_groups.sort_values(by=['parent_id'])\n",
    "top_group_id = df_groups.iloc[0]['group_id'] #Store the group id for top group \n",
    "\n",
    "df_groups.loc[df_groups['parent_id'] == 0, 'parent_id'] = top_group_id #For top level group, replace the zero value parent id with top level group id\n",
    "\n",
    "df_groups = df_groups.merge(df_groups_parent, how='inner',left_on=['parent_id'], right_on=['id']) #Add parent group name\n",
    "\n",
    "df_groups = df_groups[['group_id','group_name','parent_group_name']] #Pare down to needed columns\n",
    "\n",
    "\n",
    "#Merge the dataframes \n",
    "df_formatted = df_formatted.merge(df_groups, how='inner', on='group_id') #If you use 'outer' it will include a blank record for each group with no records\n",
    "\n",
    "print(\"Names for\",len(df_groups),\"different groups were added to the metadata records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-nation",
   "metadata": {},
   "source": [
    "### Split out custom fields\n",
    "This creates new columns for each custom field.\n",
    "\n",
    "If different groups have different custom metadata, check the output carefully to make sure things mapped properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "alleged-cartoon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom fields split out and merged\n"
     ]
    }
   ],
   "source": [
    "#The custom fields are all contained within one column called 'custom_fields'. Flatten that column and associate the values\n",
    "#with the proper article id in a new dataframe\n",
    "custom = pd.json_normalize(\n",
    "    full_records, \n",
    "    record_path =['custom_fields'], \n",
    "    meta=['id']\n",
    ")\n",
    "#This reshapes the data so that metadata field names are columns and each row is an id.\n",
    "custom = custom.pivot(index=\"id\", columns=\"name\", values=\"value\")\n",
    "\n",
    "#Merge the dataframes so that all the custom fields are visible along with all the other metadata\n",
    "df_formatted = df_formatted.merge(custom, how='outer', on='id') #Outer merge keeps records that have no custom metadata.\n",
    "\n",
    "print(\"Custom fields split out and merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-adoption",
   "metadata": {},
   "source": [
    "## Save the spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "robust-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a CSV file of all the metadata. Change the file name if necessary to match dates.\n",
    "save_file = df_formatted.to_csv('all-records-'+str(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "composite-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or save an Excel file of all the metadata. Change the file name if necessary to match dates.\n",
    "save_file = df_formatted.to_excel('all-records-'+str(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
